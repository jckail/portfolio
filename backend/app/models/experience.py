experience = {
    "prove": {
        "company": "Prove Identity",
        "title": "Staff Data Engineer",
        "date": "06/2023 - Present",
        "location": "Denver, CO",
        "highlights": [
            "Spearheading company-wide refactor from on-prem Java + Oracle to cloud-based Go + Postgres, reducing core product API response time to 120ms and operational expenses by 95%.",
            "Built AI-driven Retrieval-Augmented Generation (RAG) chatbots with Airflow, LangChain, and OpenAI, automating 150+ human-hours weekly.",
            "Deployed an event-driven data streaming platform using Go, Flink, and Kafka, migrating 1,200+ batch jobs to real-time processing, enhancing data availability from days to minutes.",
        ],
        "link": "https://www.prove.com/",
        "logoPath": "/images/companylogos/prove.svg",
        "company_description": "Prove is the modern platform for consumer identity verification. We enable businesses to securely verify customer identities in real-time, with the highest level of compliance and user experience.",
        "tech_stack": ["Airflow", "Spark", "PyTorch", "NLP", "Computer Vision"],
        "more_highlights": [
            "Spearheaded a company-wide migration from legacy Java/Oracle infrastructure to a cloud-native Go/PostgreSQL stack, cutting API response times from 30s to 12ms and slashing operational costs by 95%. Overcame challenges with service downtime, seamlessly integrating AWS services (Athena, S3, EC2) for enhanced scalability.",
            "Managed and mentored 9 data engineers and 6 data scientists, driving a 20% improvement in project delivery timelines. Collaborated with the VP of Platform Engineering on critical initiatives, serving as the primary data liaison to ensure seamless cross-departmental coordination.",
            "Engineered AI-powered RAG chatbots leveraging Airflow, LangChain, and OpenAI APIs to automate customer service workflows, reducing manual efforts by 150 hours per week. Addressed NLP challenges in ambiguous query handling, driving a 20% improvement in first-contact resolution.",
            "Deployed an event-driven data streaming platform with Go, Kafka, and Flink, transitioning 1,200+ batch jobs to real-time processing. Reduced data availability lag from days to seconds, significantly boosting dashboard accuracy and enabling data-driven decision-making for executive teams.",
            "Implemented real-time telemetry services with Prometheus, Grafana, Splunk, and AWS CloudWatch, addressing complex monitoring needs. Reduced incident response times by 40% while maintaining 99.9% system uptime for mission-critical services.",
            "Designed and developed data pipelines using Spark, Airflow, and DBT to streamline ETL processes, improving performance by 35%. Reduced report generation timelines from days to hours, enhancing reporting capabilities for product managers and business teams.",
            "Established robust data governance frameworks with Apache Atlas, ensuring full GDPR and SOC2 compliance. Implemented metadata standards, lineage tracking, and access policies, reducing audit preparation times by 30%.",
            "Optimized CI/CD pipelines for Apache Beam and Kubernetes deployments, resolving bottlenecks and reducing deployment times by 30%. Improved release frequency to support faster feature rollouts and iterative development cycles.",
            "Migrated computationally intensive SQL queries from RDS to Spark DataFrames, improving query performance by 70% and reducing compute costs by 30%. Enabled seamless processing of large datasets for business-critical applications.",
        ],
    },
    "meta": {
        "company": "Meta | Facebook",
        "title": "Senior Data Engineer",
        "date": "01/2021 - 09/2022",
        "location": "Seattle, WA + Menlo Park, CA + Remote, USA",
        "highlights": [
            "Led data engineering efforts for Facebook Public Groups and Community Chats, managing a team of 10+ engineers.",
            "Developed and deployed 100+ ML pipelines with Airflow, Spark, and PyTorch, leveraging NLP, computer vision, and user segmentation to optimize ad targeting and notifications for billions of users.",
            "Designed an automated framework to dynamically generate thousands of async Spark data pipelines, increasing compute efficiency by 66%.",
            "Re-architected petabyte-scale data models, reducing storage costs by 25%.",
        ],
        "link": "https://about.fb.com/",
        "logoPath": "/images/companylogos/meta.svg",
        "company_description": "Meta Platforms, Inc. is an American multinational technology conglomerate based in Menlo Park, California. It was founded by Mark Zuckerberg, along with his college roommates and fellow Harvard University students Eduardo Saverin, Andrew McCollum, Dustin Moskovitz and Chris Hughes, originally as TheFacebook.com—today's Facebook, a popular global social networking service.",
        "tech_stack": ["Airflow", "Spark", "PyTorch", "NLP", "Computer Vision"],
        "more_highlights": [
            "Lead data engineer for Facebook Public Groups, Community Chats, and cross-platform initiatives, leading 10+ engineers. Collaborated with data science, ML, and hardware teams to align product goals, resulting in a 20% faster delivery of cross-platform features.",
            "Designed exabyte scale data models for Community Messenger to handle multi-platform data streams, increasing user engagement by 35% across Instagram, WhatsApp, Facebook, and Quest despite complex cross-platform dependencies.",
            "Developed modular frameworks for data pipelines, streamlining data flows for thousands of engineers. Reduced integration issues by 40% and accelerated feature deployments by 25% through automation and standardization.",
            "Built telemetry systems capable of processing 10M+ events/second, improving signal quality by 20%. Developed Jinja-based monitoring tools, reducing downtime by 15% and ensuring reliable system performance.",
            "Implemented graph and entity models to support 10 billion monthly interactions, ensuring seamless experiences across Instagram, WhatsApp, Facebook, and Quest. Addressed latency and consistency challenges to maintain real-time performance.",
            "Served as the liaison between Facebook Messenger, Groups, and the early precursor to the LLaMA project, fine-tuning models for automated group management. Increased group engagement by 15% in pilot testing, paving the way for future LLM-powered features.",
            "Created from scratch QR code group invites, increasing join rates by 50%. Enabled offline engagement in multilingual regions, facilitating family reconnections and shelter logistics coordination during humanitarian efforts. Featured on Tech Crunch.",
            "Designed KPI dashboards to monitor DAUs, MAUs, and engagement trends using tools such as Tableau and internal data visualization frameworks. Enabled real-time insights, increasing engagement by 10% and retention by 12%.",
            "Designed and deployed 100+ ML pipelines using Airflow, Spark, and PyTorch, integrating NLP and computer vision models. Improved ad targeting precision and personalized notifications, driving higher engagement across billions of users.",
            "Engineered an automated framework for generating thousands of asynchronous Spark data pipelines, increasing compute efficiency by 66%. Overcame orchestration challenges, improving resource utilization and processing times.",
        ],
    },
    "deloitte": {
        "company": "Deloitte",
        "title": "Consultant - AI & Advanced Analytics",
        "date": "12/2018 - 12/2020",
        "location": "Atlanta, GA + Menlo Park, CA",
        "highlights": [
            "Automated approximately 31% of human processed healthcare claims using transformer machine learning models, saving 250,000+ hours annually.",
            "Optimized exabyte-scale video reliability metrics, reducing daily processing time by 90% while expanding metric coverage.",
            "Led 20+ consultants on Fortune 50 engagements, translating client needs into actionable requirements and ensuring timely delivery of technical solutions.",
        ],
        "link": "https://www2.deloitte.com/ch/en/pages/strategy-operations/solutions/analytics-and-cognitive.html",
        "logoPath": "/images/companylogos/deloitte.svg",
        "company_description": "Deloitte Touche Tohmatsu Limited, commonly referred to as Deloitte, is a multinational professional services network. Deloitte is one of the Big Four accounting organizations and the largest professional services network in the world by revenue and number of professionals, with headquarters in London, United Kingdom",
        "tech_stack": ["Airflow", "Spark", "PyTorch", "NLP", "Computer Vision"],
        "more_highlights": [
            "Automated 31% of healthcare claims processing with transformer-based ML models, addressing data inconsistencies and regulatory constraints. Saved 250,000+ human-hours annually and improved claim accuracy by 20%",
            "Optimized exabyte-scale video reliability metrics using distributed data processing frameworks. Cut daily processing time by 90% and expanded metric coverage across multiple product lines, improving monitoring precision.",
            "Led teams of 20+ consultants on high-profile Fortune 50 engagements, delivering advanced technical solutions aligned with client needs. Achieved 100% on-time project delivery across multiple engagements.",
            "Developed a risk detection service using machine learning algorithms to flag high-value insurance accounts. Reduced billing errors by 30% and improved revenue recovery through early detection of anomalies.",
            "Engineered NLP models inspired by Google’s Transformer architecture within six months of its release. Reduced billing errors by 20% by implementing cutting-edge language models for document processing.",
            "Implemented massively parallel data pipelines using Spark and asynchronous frameworks, reducing healthcare claim turnaround times by 40%. Improved processing efficiency for high-volume workloads.",
            "Re-architected live video infrastructure to align with emerging short-form content trends, saving $100 million and 14 months of development time. Ensured seamless adoption of new formats across the platform.",
            "Optimized caching strategies, leveraging Redis and CDN-layer optimizations to cut costs by $10 million annually. Enhanced content delivery efficiency and reduced latency for high-traffic web services.",
            "Built predictive analytics models for server uptime using time-series forecasting techniques, improving video delivery reliability by 30% and enhancing user experience through proactive maintenance.",
            "Revamped A/B testing frameworks for billions of daily users, resolving data inconsistencies and enabling more precise feature rollouts. Accelerated data-driven decision-making with improved statistical significance tracking.",
            "Leveraged Python, SQL, Java, TensorFlow, PyTorch, Spark, Airflow, Docker, and Kubernetes to develop and deploy scalable technical solutions. Delivered projects across machine learning, real-time analytics, and data pipeline automation for Fortune 50 clients.",
        ],
    },
    "wide_open_west": {
        "company": "Wide Open West",
        "title": "Lead Data Scientist",
        "date": "11/2017 - 12/2018",
        "location": "Denver, CO",
        "highlights": [
            "Built Machine Learning applications using custom classification and churn models, driving a 22% YoY increase in customer package upgrades.",
            "Led a team of 5 data practitioners, providing BI and data insights to sales, product, and engineering teams company-wide.",
        ],
        "link": "https://www.wowway.com/",
        "logoPath": "/images/companylogos/wow.svg",
        "company_description": "Wide Open West is the sixth largest cable operator in the United States. The company offers landline telephone, Cable Television, and broadband Internet services",
        "tech_stack": ["Airflow", "Spark", "PyTorch", "NLP", "Computer Vision"],
        "more_highlights": [
            "Built machine learning models, including custom classification and churn prediction algorithms (e.g., logistic regression, K-means clustering). Increased customer package upgrades by 22% YoY and reduced churn by 15%.",
            "Managed a team of 5 data practitioners, providing BI and insights to sales, product, and engineering teams. Established KPIs and standardized reporting through governance committees, driving a 10% increase in sales performance.",
            "Developed a Kafka-powered real-time analytics platform, streaming data from field technicians and delivering instant job updates via a custom web portal. Reduced service completion times by 40%, replacing 20-minute phone calls with real-time notifications.",
            "Built scalable, cloud-based data solutions leveraging AWS services (SageMaker, S3, Redshift, and Athena). Improved data accessibility and reduced query times by 30% across sales and operations teams.",
            "Automated marketing campaigns using SendGrid to engage at-risk customers, reducing churn by 15%. Implemented multi-channel unsubscribe mechanisms, ensuring 100% compliance with communication preferences.",
            "Delivered geospatial insights using GIS tools to support sales in Arkansas and Alabama. Optimized resource allocation down to the city block level, driving an 18% increase in sales conversions and empowering door-to-door teams.",
            "Developed a dynamic revenue forecasting tool using Python and SQL to set bonus targets and calculate commissions by territory. Increased sales velocity and retention, driving a 12% increase in quarterly sales.",
            "Created a sales funnel dashboard with Tableau to monitor add-on targets, conversions, and installations. Identified millions in unrealized losses, leading to strategic reallocations and improved market performance.",
            "Applied machine learning models and geospatial analytics to optimize network NUC performance, reducing infrastructure build-out costs by 25%. Implemented continuous deployment with GitHub, ensuring code quality through design principles and best practices.",
            "Technologies Used: Python, Java, JavaScript, SQL, Flask, AWS (SageMaker, S3, EC2, Redshift, Athena), Apache Kafka, SendGrid, GIS tools.",
        ],
    },
    "common_spirit_health": {
        "company": "Common Spirit Health",
        "title": "Data Engineer - Business Intelligence",
        "date": "09/2016 - 11/2017",
        "location": "Denver, CO",
        "highlights": [
            "Architected and delivered new rest APIs and data lakes, improving data processing time for external partner data products from 7 days to 5 minutes."
        ],
        "link": "https://www.commonspirit.org/",
        "logoPath": "/images/companylogos/commonspirit.svg",
        "company_description": "CommonSpirit Health is a nonprofit, Catholic health system dedicated to advancing health for all people. It was created in February 2019 through the alignment of Catholic Health Initiatives and Dignity Health. CommonSpirit Health is the largest nonprofit health system in the U.S. with more than 1,000 care sites in 21 states.",
        "tech_stack": ["Airflow", "Spark", "PyTorch", "NLP", "Computer Vision"],
        "more_highlights": [
            "",
            "Architected and delivered new REST APIs and data lakes, reducing external partner data processing time from 7 days to 5 minutes and improving partner satisfaction by 30%. Enhanced data accessibility and scalability through efficient data structures.",
            "Managed tier-one vendor data extracts containing patient records, financial data, and ICD codes. Improved extract performance by 40% and ensured 100% HIPAA compliance through encryption and access control measures.",
            "Developed a data quality dashboard using Tableau and Python, enabling real-time failure detection. Reduced issue resolution time by 50% and improved overall data integrity and operational reliability.",
            "Conducted advanced data analysis using Dimensional Fact Models in SMP and MPP environments, improving query performance by 35%. Delivered actionable insights to executives, enhancing decision-making processes.",
            "Developed flexible big data extracts and real-time CDC lakes using AWS Redshift and Kafka. Enabled faster product delivery, reducing time to market by 20%.",
            "Designed complex data models for highly sensitive UII data, employing encryption and role-based access control. Ensured data security while supporting high-stakes analytics and compliance use cases.",
            "Created analytics dashboards using Qlik Sense, Tableau, and Python to track key metrics. Delivered actionable insights to executives, increasing reporting efficiency by 25% and enhancing operational visibility.",
            "Developed optimized storage and compute solutions, reducing third-party vendor data costs by 45%. Earned recognition from the VP of Business Intelligence for faster issue resolution and improved efficiency.",
            "Migrated data from relational to columnar formats (e.g., Parquet) using Redshift, improving query speed by 40% and enabling large-scale data processing for analytics.",
            "Developed machine learning pipelines using Python and SQL to forecast hospital procedures, billing, and staffing. Reduced billing turnaround from 14 to 10 days, optimized surgical room usage by 3000%, and enabled predictive staffing to improve patient care.",
            "Worked with orchestration tools similar to Airflow and utilized cloud technologies (Microsoft Azure and AWS) for data infrastructure, achieving seamless cloud operations and reducing deployment times.",
            "Leveraged strong Python and SQL expertise for ETL/ELT processes, developing scalable solutions with continuous improvement and adhering to best coding practices.",
        ],
    },
    "acustream": {
        "company": "AcuStream (R1)",
        "title": "Software Engineer - Data",
        "date": "04/2013 - 09/2016",
        "location": "Boulder, CO",
        "highlights": [
            "Built a custom invoicing system leveraging rule-based algorithms and machine learning, driving over $300M in annual recurring revenue."
        ],
        "link": "https://www.r1rcm.com/",
        "logoPath": "/images/companylogos/r1.svg",
        "company_description": "R1 RCM is an American healthcare revenue cycle management company servicing hospitals, health systems and physician groups across the United States. Headquartered in Chicago, Illinois, R1 RCM is publicly traded on the NASDAQ.",
        "tech_stack": ["Airflow", "Spark", "PyTorch", "NLP", "Computer Vision"],
        "more_highlights": [
            "Developed a custom invoicing system using Python and rule-based algorithms with ML components, generating $300M+ in annual revenue. Improved invoice accuracy by 35% and automated processes to reduce manual effort by 60%.",
            "Partnered with executives on high-impact initiatives, driving a 200% increase in revenue and boosting client retention by 25%. Optimized operational strategies, improving profit margins from 41% to 78%.",
            "Collaborated with CFOs and revenue cycle directors at leading healthcare systems to implement automated reconciliation processes. Cut reconciliation times by 50% and achieved 98% client satisfaction through scalable data solutions.",
            "Built a financial reconciliation platform with Django and Celery, automating line-item invoicing and scheduling. Improved billing speed by 40% and streamlined operations despite complex client requirements.",
            "Led infrastructure migration to AWS, moving PostgreSQL to RDS and compute workloads to EC2. Reduced query latency by 30% and cut infrastructure costs by 25%. Implemented IAM-based security, improving compliance audit performance by 20%.",
            "Refactored 100+ code modules into optimized Python, leveraging multithreading, hashing, and compression techniques. Boosted system efficiency by 45%, resolving bottlenecks in data processing workflows.",
            "Diagnosed and resolved issues in legacy Java applications, reducing downtime incidents by 15%. Enhanced UI responsiveness by 20% through performance optimizations in JavaScript.",
            "Created a Django-powered cron job system with Celery for task orchestration and real-time tracking. Increased task completion rates by 30% through automated monitoring and recovery mechanisms.",
            "Migrated clients from SFTP to real-time APIs with Flask and Apache Kafka, reducing data delivery times by 50%. Enhanced data accessibility, streamlining client operations and improving service quality.",
            "Developed fault-tolerant systems with Spring Boot, implementing state consistency mechanisms such as transaction rollbacks. Reduced system fault impact by 35%, ensuring high availability during critical operations.",
            "Supported pre-sales efforts and customer onboarding with on-site integrations, accelerating implementation timelines by 20% and enhancing customer satisfaction.",
            "Technologies Used: Python, Java, JavaScript, SQL, Django, Flask, Celery, AWS (RDS, EC2, IAM), Apache Kafka, Spring Boot, PostgreSQL.",
        ],
    },
}
